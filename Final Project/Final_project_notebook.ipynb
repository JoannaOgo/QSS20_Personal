{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afde981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.23.6-cp39-none-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.23.6 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.23.6-py3-none-macosx_10_9_x86_64.whl.metadata (1.3 kB)\n",
      "Downloading PyMuPDF-1.23.6-cp39-none-macosx_10_9_x86_64.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.23.6-py3-none-macosx_10_9_x86_64.whl (30.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/30.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.23.6 PyMuPDFb-1.23.6\n"
     ]
    }
   ],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "## nltk imports\n",
    "#!pip install nltk # can install on terminal or by uncommenting this line\n",
    "#import nltk; nltk.download('punkt'); nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## lda\n",
    "#!pip install gensim # can install by uncommenting this line\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## visualizing LDA--likely need to install\n",
    "#!pip install pyLDAvis # can install by uncommenting this line\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "## print mult things\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## random\n",
    "import random\n",
    "import string; punctlist = [char for char in string.punctuation] # list of english punctuation marks\n",
    "\n",
    "!pip install PyMuPDF\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fcdc83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User Name: AYCEID ServicetoService\\nDate and Time: Monday, May 11, 2020 7:43:00 AM EDT\\nJob Number: 116541938\\n\\n1. Barclays to pay £290m penalties as Bob Diamond forgoes bonus\\n\\nDocument (1)\\n\\n| About LexisNexis | Privacy Policy | Terms & Conditions | Copyright © 2020 LexisNexis\\n\\nAYCEID ServicetoService\\n\\n\\x0cBarclays to pay £290m penalties as Bob Diamond forgoes bonus\\n\\nIndependent.co.uk\\n\\nJune 27, 2012 Wednesday 6:18 PM GMT\\n\\nCopyright 2012 Independent News & Media plc All Rights Reserved\\n\\nSection: BUSINESS NEWS\\n\\nLength: 718 words\\n\\nByline: Peter Cripps\\nBody\\n\\nThe banking industry was engulfed in a fresh scandal today after Barclays paid £290 million to settle claims that it \\nused underhand tactics to try to rig financial markets. \\n\\nThe penalties from UK and US regulators, including a record £59.5 million fine from the Financial Services Authority \\n(FSA),  followed  allegations  it  manipulated  Libor  and  Euribor  interbank  lending,  which  govern  the  rates  at  which \\nbanks are prepared to lend to each other in the wholesale money markets. \\n\\nIn the depths of the financial crisis, Barclays gave false information about the interest rates it had to pay to borrow \\nmoney in an effort to paint a false picture of its health to markets. \\n\\nChief executive Bob Diamond, who was in charge of Barclays Capital at the time the breaches occurred between \\n2005 and 2009, apologised and said he and three other key executives would waive their bonuses for this year. \\n\\nA trail of emails and messages disclosed by the FSA showed how traders broke so-called Chinese Walls, which are \\ndesigned to avoid conflicts of interest within financial firms, as they requested Barclays make changes to the Libor \\nrate in a bid to boost their profits. \\n\\nIn one request for a change to the Libor rate, a trader said: \"Coffees will be coming your way either way, just to say \\nthank you for your help in the past few weeks\". To which the Barclays submitter responded: \"Done, for you big boy.\" \\n\\nAfter  one  submitter  of  information  responded  favourably  to  a  trader\\'s  request  to  lower  a  closely-watched  interest \\nrate, the trader came back: \"When I retire and write a book about this business your name will be written in golden \\nletters.\" \\n\\nThe scandal is another blow to the beleaguered banking sector as it battles to restore its tarnished image in the \\nwake of the financial crisis, the scandal of mis-sold PPI and the computer problems at RBS which froze millions out \\nof their accounts. \\n\\nBarclays  is  the  first  major  financial  institution  to  settle  with  regulators  following  a  wide-ranging  probe  that  has \\nspanned North America and Europe. \\n\\nMr Diamond said: \"I am sorry that some people acted in a manner not consistent with our culture and values. \\n\\nAYCEID ServicetoService\\n\\n\\x0cBarclays to pay £290m penalties as Bob Diamond forgoes bonus\\n\\nPage 2 of 3\\n\\n\"The events which gave rise to today\\'s resolutions relate to past actions which fell well short of the standards to \\nwhich Barclays aspires in the conduct of its business. \\n\\n\"Nothing is more important to me than having a strong culture at Barclays.\" \\n\\nMP  John  Mann,  a  member  of  the  Treasury  Select  Committee,  called  for  Mr  Diamond  and  RBS  chief  executive \\nStephen Hester to be \"de-bonused\" by taking away the biggest bonus they received over the last three years from \\ntheir pay. \\n\\nHe said: \"Bank bosses have been happy to justify their huge bonuses, claiming it is needed to reward success. \\n\\n\"As we have a meltdown in RBS consumer services and the equivalent of fraud in fiddling interest rates at Barclays, \\nthe abject failure of both of these bankers needs to be recognised in their annual pay.\" \\n\\nThe fine from the FSA would have been £85 million had Barclays not co-operated. \\n\\nBarclays  also  agreed  to  settle  a  penalty  of  200  million  US  dollars  (£128.2  million)  with  the  Commodity  Futures \\nTrading Commission (CFTC) and 160 million US dollars (£102.5 million) to the US Department of Justice (DoJ). \\n\\nThe  breaches  mainly  occurred  between  2005  and  2008,  a  period  when  John  Varley  was  at  the  helm  of  the \\ncompany and Mr Diamond led Barclays Capital - where the traders worked. \\n\\nMr Diamond took a £2.7 million cash bonus last year despite widespread criticism that his pay failed to reflect the \\nstruggling performance of the bank. \\n\\nOverall, his package was worth £17.7 million including a £5.7 million tax payment made on his behalf. \\n\\nOther  key  executives  to  have  waived  their  bonuses  were  Jerry  del  Missier  and  Rich  Ricci,  who  both  helped  run \\nBarCap, and chief financial officer Chris Lucas. \\n\\nThe FSA said Barclays\\' breaches of its requirements involved a significant number of employees and occurred over \\na number of years. \\n\\nTracey McDermott, the FSA\\'s acting director of enforcement and financial crime, said: \"Barclays\\' misconduct was \\nserious, widespread and extended over a number of years. \\n\\n\"Making submissions to try to benefit trading positions is wholly unacceptable. \\n\\n\"Barclays\\' behaviour threatened the integrity of the rates with the risk of serious harm to other market participants.\" \\n\\nPA\\n\\nClassification\\n\\nLanguage: ENGLISH\\n\\nPublication-Type: Web Publication\\n\\nJournal Code: WEBI\\n\\nAYCEID ServicetoService\\n\\n\\x0cBarclays to pay £290m penalties as Bob Diamond forgoes bonus\\n\\nPage 3 of 3\\n\\nSubject: NEGATIVE PERSONAL NEWS (90%); BUSINESS NEWS (90%); INTEREST RATES (89%); LONDON \\nINTERBANK OFFERED RATES (89%); BANKING & FINANCE SECTOR PERFORMANCE (89%); ECONOMIC \\nCRISIS (75%); SELF REGULATING ORGANIZATIONS (75%); FINES & PENALTIES (74%); MARKET \\nMANIPULATION (74%); FALSE STATEMENTS (73%); EXECUTIVES (70%); EURO (68%)\\n\\nCompany:  BARCLAYS PLC (92%);  BARCLAYS CAPITAL INC (56%)\\n\\nTicker: TAPR (NASDAQ) (92%); BCS (NYSE) (92%); BARC (LSE) (92%)\\n\\nIndustry: NAICS522110 COMMERCIAL BANKING (92%); SIC6029 COMMERCIAL BANKS, NEC (92%); \\nBANKING & FINANCE (91%); BANKING & FINANCE REGULATION & POLICY (90%); BANKING & FINANCE \\nSECTOR PERFORMANCE (89%); INTEREST RATES (89%); LONDON INTERBANK OFFERED RATES (89%); \\nMARKET MANIPULATION (74%); RETAIL & WHOLESALE TRADE (70%); EURO (68%); INTERBANK LENDING \\n(53%)\\n\\nGeographic: EUROPE (79%); NORTH AMERICA (79%)\\n\\nLoad-Date: June 28, 2012\\n\\nEnd of Document\\n\\nAYCEID ServicetoService\\n\\n\\x0c'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "extract_text(\"Data/Barclays/1st indy Barclays to pay £290m penalties as Bob Diamond forgoes bonus.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38182913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b79b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(folder_path):\n",
    "    data = {}\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            if file.endswith('.pdf'):\n",
    "                with fitz.open(file_path) as pdf_document:\n",
    "                    text = \"\"\n",
    "                    for page_num in range(pdf_document.page_count):\n",
    "                        page = pdf_document[page_num]\n",
    "                        text += page.get_text()\n",
    "\n",
    "                data[file] = text\n",
    "            else:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data[file] = f.read()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77fadda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    porter = PorterStemmer()\n",
    "    tokens = [porter.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7fa4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform text analysis\n",
    "def text_analysis(data):\n",
    "    # Convert data to a pandas DataFrame\n",
    "    df = pd.DataFrame(list(data.items()), columns=['File', 'Content'])\n",
    "\n",
    "    # Preprocess text\n",
    "    df['Processed_Content'] = df['Content'].apply(preprocess_text)\n",
    "\n",
    "    # Create a dictionary and a corpus for the LDA model\n",
    "    dictionary = corpora.Dictionary(df['Processed_Content'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in df['Processed_Content']]\n",
    "\n",
    "    # Apply Latent Dirichlet Allocation (LDA) for topic modeling\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
    "\n",
    "    # Add topics to the DataFrame\n",
    "    df['Topic'] = df['Processed_Content'].apply(lambda x: lda_model[dictionary.doc2bow(x)][0][0])\n",
    "\n",
    "    return df, lda_model, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536b5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize topics using pyLDAvis\n",
    "def visualize_topics(lda_model, dictionary, df):\n",
    "    vis_data = gensimvis.prepare(lda_model,\n",
    "                                 df['Processed_Content'].apply(lambda x: dictionary.doc2bow(x)), dictionary)\n",
    "    pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1b7d48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "folder_path = '.../Data/Barclays'\n",
    "data = read_data(folder_path)\n",
    "print(data)\n",
    "#df, lda_model, dictionary = text_analysis(data)\n",
    "#visualize_topics(lda_model, dictionary, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a564aea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5daa4f9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting slate3k\n",
      "  Downloading slate3k-0.5.3-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting pdfminer3k (from slate3k)\n",
      "  Downloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ply (from pdfminer3k->slate3k)\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ply, pdfminer3k, slate3k\n",
      "Successfully installed pdfminer3k-1.3.4 ply-3.11 slate3k-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install slate3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad68ad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No non-empty text found in the PDFs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import slate3k as slate\n",
    "\n",
    "# Folder path containing PDF files\n",
    "folder_path = \"/Users/angel_jo/Documents/GitHub/QSS20_Personal/Final Project/Data\"\n",
    "\n",
    "# Function to extract text from PDF files using slate3k\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        doc = slate.PDF(file)\n",
    "        for page in doc:\n",
    "            text += page.text\n",
    "    return text\n",
    "\n",
    "# Read text from PDF files in the specified folder\n",
    "pdf_texts = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(folder_path, filename)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            pdf_texts.append(text)\n",
    "            print(f\"Length of text in {filename}: {len(text)}\")\n",
    "\n",
    "# Create a DataFrame from the extracted text\n",
    "df = pd.DataFrame({\"Text\": pdf_texts})\n",
    "\n",
    "# Check if there is non-empty text\n",
    "if not df.empty:\n",
    "    # Create a Document-Term Matrix (DTM)\n",
    "    list_stopwords = stopwords.words(\"english\")\n",
    "    custom_words_to_add = ['bank', 'barclays', 'telegraph', 'guardian', 'independent', 'lexisnexis']\n",
    "    list_stopwords_new = list_stopwords + custom_words_to_add\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=list_stopwords_new)\n",
    "    dtm = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "    # Example: Display the DTM\n",
    "    print(dtm)\n",
    "else:\n",
    "    print(\"No non-empty text found in the PDFs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed025237",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textract\n",
      "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
      "Collecting argcomplete~=1.10.0 (from textract)\n",
      "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting beautifulsoup4~=4.8.0 (from textract)\n",
      "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting chardet==3.* (from textract)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docx2txt~=0.8 (from textract)\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting extract-msg<=0.29.* (from textract)\n",
      "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfminer.six==20191110 (from textract)\n",
      "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting python-pptx~=0.6.18 (from textract)\n",
      "  Downloading python_pptx-0.6.23-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting six~=1.12.0 (from textract)\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting SpeechRecognition~=3.8.1 (from textract)\n",
      "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xlrd~=1.2.0 (from textract)\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodome (from pdfminer.six==20191110->textract)\n",
      "  Downloading pycryptodome-3.19.0-cp35-abi3-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting sortedcontainers (from pdfminer.six==20191110->textract)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from beautifulsoup4~=4.8.0->textract) (2.3.2.post1)\n",
      "Collecting imapclient==2.1.0 (from extract-msg<=0.29.*->textract)\n",
      "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m239.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting olefile>=0.46 (from extract-msg<=0.29.*->textract)\n",
      "  Downloading olefile-0.46.zip (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tzlocal>=2.1 (from extract-msg<=0.29.*->textract)\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting compressed-rtf>=1.0.6 (from extract-msg<=0.29.*->textract)\n",
      "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ebcdic>=1.1.1 (from extract-msg<=0.29.*->textract)\n",
      "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lxml>=3.1.0 (from python-pptx~=0.6.18->textract)\n",
      "  Downloading lxml-4.9.3.tar.gz (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=3.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-pptx~=0.6.18->textract) (8.2.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx~=0.6.18->textract)\n",
      "  Downloading XlsxWriter-3.1.9-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodome-3.19.0-cp35-abi3-macosx_10_9_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, lxml, olefile\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=c146e3017f8f57f14746708dee89c60e69de0db0f36bab711cb0832af1a81d0a\n",
      "  Stored in directory: /Users/angel_jo/Library/Caches/pip/wheels/40/75/01/e6c444034338bde9c7947d3467807f889123465c2371e77418\n",
      "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6185 sha256=974ba2dbcab5be588a2b8061c47c3be677b20a1af3ee5280c99a562acce18f99\n",
      "  Stored in directory: /Users/angel_jo/Library/Caches/pip/wheels/e4/67/e4/ba2159853bdd0fe99330aa1e384915108143a5370686ea446f\n",
      "  Building wheel for lxml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lxml: filename=lxml-4.9.3-cp39-cp39-macosx_10_9_x86_64.whl size=1881202 sha256=05e66bcdee84fcc3d2eacbc6028309f137511909c2127edae9c8aeff4d0d6f02\n",
      "  Stored in directory: /Users/angel_jo/Library/Caches/pip/wheels/5c/05/aa/530f84480d476c5bb9ea09877eea78fb144ec047fbb00ee2ca\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=56fa53f5f3c021856079e49da55694ba0aa4151266b7030097e20a9ed114a83f\n",
      "  Stored in directory: /Users/angel_jo/Library/Caches/pip/wheels/64/b8/ba/ebba30390fbd997074f35e42a842ce3fd933213cac8753414e\n",
      "Successfully built docx2txt compressed-rtf lxml olefile\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: SpeechRecognition, sortedcontainers, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, tzlocal, six, pycryptodome, olefile, lxml, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.11.1\n",
      "    Uninstalling beautifulsoup4-4.11.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.11.1\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20221105\n",
      "    Uninstalling pdfminer.six-20221105:\n",
      "      Successfully uninstalled pdfminer.six-20221105\n",
      "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.1.9 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 lxml-4.9.3 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.19.0 python-pptx-0.6.23 six-1.12.0 sortedcontainers-2.4.0 textract-1.6.5 tzlocal-5.2 xlrd-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9563ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No non-empty text found in the PDFs.\n"
     ]
    }
   ],
   "source": [
    "# Folder path containing PDF files\n",
    "folder_path = \"/Users/angel_jo/Documents/GitHub/QSS20_Personal/Final Project/Data\"\n",
    "\n",
    "# Function to extract text from PDF files using Textract\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        text = textract.process(pdf_path).decode(\"utf-8\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Read text from PDF files in the specified folder\n",
    "pdf_texts = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(folder_path, filename)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            pdf_texts.append(text)\n",
    "            print(f\"Length of text in {filename}: {len(text)}\")\n",
    "\n",
    "# Create a DataFrame from the extracted text\n",
    "df = pd.DataFrame({\"Text\": pdf_texts})\n",
    "\n",
    "# Check if there is non-empty text\n",
    "if not df.empty:\n",
    "    # Create a Document-Term Matrix (DTM)\n",
    "    list_stopwords = stopwords.words(\"english\")\n",
    "    custom_words_to_add = ['bank', 'barclays', 'telegraph', 'guardian', 'independent', 'lexisnexis']\n",
    "    list_stopwords_new = list_stopwords + custom_words_to_add\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=list_stopwords_new)\n",
    "    dtm = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "    # Example: Display the DTM\n",
    "    print(dtm)\n",
    "else:\n",
    "    print(\"No non-empty text found in the PDFs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8a323ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No non-empty text found in the PDFs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Folder path containing PDF files\n",
    "folder_path = \"/Users/angel_jo/Documents/GitHub/QSS20_Personal/Final Project/Data\"\n",
    "\n",
    "# Function to extract text from PDF files using PyMuPDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf_doc:\n",
    "        for page_num in range(pdf_doc.page_count):\n",
    "            page = pdf_doc[page_num]\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Read text from PDF files in the specified folder\n",
    "pdf_texts = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(folder_path, filename)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            pdf_texts.append(text)\n",
    "            print(f\"Length of text in {filename}: {len(text)}\")\n",
    "\n",
    "# Create a DataFrame from the extracted text\n",
    "df = pd.DataFrame({\"Text\": pdf_texts})\n",
    "\n",
    "# Check if there is non-empty text\n",
    "if not df.empty:\n",
    "    # Create a Document-Term Matrix (DTM)\n",
    "    list_stopwords = stopwords.words(\"english\")\n",
    "    custom_words_to_add = ['apartment', 'new york', 'nyc', 'bronx', 'brooklyn', 'manhattan', 'queens', 'staten island']\n",
    "    list_stopwords_new = list_stopwords + custom_words_to_add\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=list_stopwords_new)\n",
    "    dtm = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "    # Example: Display the DTM\n",
    "    print(dtm)\n",
    "else:\n",
    "    print(\"No non-empty text found in the PDFs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7452342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pdf2image) (8.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pytesseract) (23.0)\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pytesseract, pdf2image\n",
      "Successfully installed pdf2image-1.16.3 pytesseract-0.3.10\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2image pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69f223ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No non-empty text found in the PDFs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "# Set the path to the Tesseract executable (change accordingly)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Folder path containing PDF files\n",
    "folder_path = \"/Users/angel_jo/Documents/GitHub/QSS20_Personal/Final Project/Data\"\n",
    "\n",
    "# Function to extract text from PDF files using OCR\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    text = \"\"\n",
    "    for i, image in enumerate(images):\n",
    "        text += pytesseract.image_to_string(image)\n",
    "    return text\n",
    "\n",
    "# Read text from PDF files in the specified folder\n",
    "pdf_texts = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(folder_path, filename)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            pdf_texts.append(text)\n",
    "            print(f\"Length of text in {filename}: {len(text)}\")\n",
    "\n",
    "# Create a DataFrame from the extracted text\n",
    "df = pd.DataFrame({\"Text\": pdf_texts})\n",
    "\n",
    "# Check if there is non-empty text\n",
    "if not df.empty:\n",
    "    # Create a Document-Term Matrix (DTM)\n",
    "    list_stopwords = stopwords.words(\"english\")\n",
    "    custom_words_to_add = ['apartment', 'new york', 'nyc', 'bronx', 'brooklyn', 'manhattan', 'queens', 'staten island']\n",
    "    list_stopwords_new = list_stopwords + custom_words_to_add\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=list_stopwords_new)\n",
    "    dtm = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "    # Example: Display the DTM\n",
    "    print(dtm)\n",
    "else:\n",
    "    print(\"No non-empty text found in the PDFs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea95699",
   "metadata": {},
   "source": [
    "## Assuming text model and visualisation worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at pocentage drop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
